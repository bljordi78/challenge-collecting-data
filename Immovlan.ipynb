{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries to run the code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseURL():\n",
    "    codes = pd.read_csv(\"downloaded_files/code-postaux-belge.csv\",sep=\";\")\n",
    "    codes = codes[['Code', 'Localite']]\n",
    "    codes['Localite'] = codes['Localite'].str.replace(' ', '-', regex=False).str.replace(\"'\", \"\", regex=False)\n",
    "    \n",
    "    codes = codes.apply(lambda row: str(row.Code) + '-' + row.Localite, axis=1)\n",
    "    \n",
    "    rootURL = 'https://immovlan.be/fr/immobilier?transactiontypes=a-vendre&propertytypes=maison,appartement&towns='\n",
    "\n",
    "    baseURLs = []\n",
    "    for code in codes:\n",
    "        url = rootURL + code + '&page='\n",
    "        baseURLs.append(url)\n",
    "    \n",
    "    return baseURLs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd013894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing_data():\n",
    "    base = baseURL()[:10]  # limit for testing purposes\n",
    "    data = []\n",
    "\n",
    "    for b in tqdm(base, desc=\"Scraping URLs\"):\n",
    "        url = b + '1'\n",
    "        driver = get_driver()\n",
    "        driver.get(url)\n",
    " \n",
    "        try:\n",
    "            agree_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]/span'))\n",
    "            )\n",
    "            agree_button.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        total_results = None\n",
    "        total_pages = None\n",
    "\n",
    "        result_div = soup.find('div', class_='col-12 mb-2')\n",
    "        if result_div:\n",
    "            text = result_div.text.strip()\n",
    "            match = re.search(r'^\\d+', text)\n",
    "            if match:\n",
    "                total_results = int(match.group())\n",
    "                total_pages = math.ceil(total_results / 20)\n",
    "\n",
    "        data.append({\n",
    "            'url': b,\n",
    "            'total_results': total_results,\n",
    "            'total_pages': total_pages\n",
    "        })\n",
    "\n",
    "        # Random delay to mimic human behavior\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Optional save to CSV\n",
    "    df.to_csv(\"Scrape_TotalPages.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd08a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ads():\n",
    "\n",
    "    pages = scrape_listing_data()\n",
    "    links = []\n",
    "\n",
    "    for i in tqdm(range(5), desc=\"Scraping page links\"):\n",
    "#   for i in range(5):        #range(len(ad))\n",
    "        ad = pages.iloc[i]\n",
    "        \n",
    "\n",
    "        for p in range(1, ad['total_pages']+1):\n",
    "            url = ad['url'] + str(p) \n",
    "\n",
    "            driver = get_driver()\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the button to be clickable and click it\n",
    "            try:\n",
    "                agree_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]/span'))\n",
    "                )\n",
    "                url = agree_button.click()\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                if '/detail/' in a['href']:       \n",
    "                    if a['href'] not in links:\n",
    "                        links.append(a['href'])\n",
    "\n",
    "            time.sleep(random.uniform(1, 3))  # polite delay\n",
    "\n",
    "    driver.quit()\n",
    "    return links\n",
    "      #  print(f\"Total links scraped: {len(links)}\")\n",
    "      #  print(*links, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ads()\n",
    "print(f\"Total links scraped: {len(links)}\")\n",
    "print(*links, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4947d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ad_URL':links})\n",
    "df.to_csv(\"Ad_listing.csv\", index=False)\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe850ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "\n",
    "#for i in tqdm(range(len(df)), desc=\"Adverts scraped:\"):\n",
    "for i in tqdm(range(500,550), desc=\"Adverts scraped:\"):\n",
    "    try:\n",
    "        url = df.loc[i, \"ad_URL\"]    \n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")     \n",
    "\n",
    "        # First extracting isolated pieces in container\n",
    "        title = soup.find('span', class_=\"detail__header_title_main\")\n",
    "        ref = soup.find('span', class_=\"vlancode\")\n",
    "        price = soup.find('span', class_=\"detail__header_price_data\")    \n",
    "        address_block = soup.find('div', class_='d-lg-block d-none')\n",
    "        description = soup.find('div', class_=\"dynamic-description active\")\n",
    "\n",
    "        # then cleaning the data\n",
    "        title = title.contents[0].strip() if title else \"\"\n",
    "        ref = ref.text.strip() if ref else \"\"\n",
    "        if price:\n",
    "            price = price.text.strip() \n",
    "            price = re.sub(r\"[^\\d]\", \"\", price)            \n",
    "        else: \"\"     \n",
    "        if address_block:\n",
    "            spans = address_block.find_all('span')\n",
    "            address = spans[0].text.strip() \n",
    "            zip = spans[1].text.strip()\n",
    "        else:\n",
    "            address = \"\"\n",
    "            zip = \"\"\n",
    "        description  = description.text.strip() if description else \"\"\n",
    "\n",
    "        # and adding it to the dataframe\n",
    "        df.loc[i, \"Ref\"] = ref\n",
    "        df.loc[i, \"Titre\"] = title\n",
    "        df.loc[i, \"Prix\"] = price\n",
    "        df.loc[i, \"Addresse\"] = address\n",
    "        df.loc[i, \"Localite\"] = zip \n",
    "        df.loc[i, \"Description\"] = description       \n",
    "    \n",
    "        # then all relevant details are in the class general-info w-100\n",
    "        # iterating through all div, p, h4 to retrieve the fields\n",
    "        # keeping the html labels as codes and retriving their values to add to the dataframe\n",
    "        info_section = soup.find(\"div\", class_=\"general-info w-100\")\n",
    "        if info_section:\n",
    "            for data_row in info_section.find_all(\"div\", class_=\"data-row-wrapper\"):\n",
    "                for item in data_row.find_all(\"div\"):\n",
    "                    h4 = item.find(\"h4\")\n",
    "                    p = item.find(\"p\")\n",
    "                    if h4 and p:\n",
    "                        label = h4.get_text(strip=True)\n",
    "                        value = p.get_text(strip=True)\n",
    "                        # Optional: shorten label or sanitize column name\n",
    "                        label_clean = re.sub(r\"[^\\w\\s]\", \"\", label).strip().replace(\" \", \"_\")\n",
    "                        df.loc[i, label_clean] = value\n",
    "    \n",
    "    except: print(f\"No container found for index {i} â€” {url}\")\n",
    "\n",
    "driver.quit()\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
